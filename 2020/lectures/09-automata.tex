\documentclass[11pt,twoside]{scrartcl}
%\documentclass[11pt,twoside]{article}

%opening
\newcommand{\lecid}{15-316}
\newcommand{\leccourse}{Software Foundations of Security and Privacy}
\newcommand{\lecdate}{} %e.g. {October 21, 2013}
\newcommand{\lecnum}{9}
\newcommand{\lectitle}{Security Automata \& Instrumentation}
\newcommand{\lecturer}{Matt Fredrikson}
\newcommand{\lecurl}{https://15316-cmu.github.io/index}

\usepackage{varwidth}
\usepackage{lecnotes}
\usepackage[irlabel]{bugcatch}

\usepackage{tikz}
\usetikzlibrary{automata,shapes,positioning,matrix,shapes.callouts,decorations.text,patterns,trees,backgrounds}

% \usepackage[bracketinterpret,seqinfers,sidenotecalculus]{logic}
% \newcommand{\I}{\interpretation[const=I]}

% \newcommand{\bebecomes}{\mathrel{::=}}
% \newcommand{\alternative}{~|~}
% \newcommand{\asfml}{F}
% \newcommand{\bsfml}{G}
% \newcommand{\cusfml}{C}
% \def\leftrule{L}%
% \def\rightrule{R}%

\begin{document}

\newcommand{\atrace}{\sigma}%
%% the standard interpretation naming conventions
\newcommand{\stdI}{\dTLint[state=\omega]}%
\newcommand{\Ip}{\dTLint[trace=\atrace]}%
\newcommand{\ws}{\omega}\newcommand{\wt}{\nu}% 

\maketitle
\thispagestyle{empty}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction \& Recap}

We began last lecture by adding indirect control flow to our language, in the form of the instruction $\pifjump{\ivr}{\astrm}$.
We then returned to our sandboxing approach for untrusted program $\asprg$, and considered what could happen now that indirect jumps are in the language. Assuming that we are using SFI to enforce a sandboxing policy, there is still no way for $\asprg$ to read or write memory outside the sandbox. Is this true? Consider the following situation, where we can assume that SFI has been applied to the untrusted $\asprg$ starting at command 20.
\[
\begin{array}{l}
\phantom{\asprg\Big\{\ }
\begin{array}{ll}
& \vdots \\
\keywordfont{10:} & \pumod{z}{\pderef{x}} \\
\keywordfont{11:} & \pifjump{i\ge0}{y} \\
& \vdots \\
\end{array}
\\
\asprg\left\{
\begin{array}{ll}
\keywordfont{20:} & \pumod{i}{0} \\
\keywordfont{21:} & \pumod{x}{\textit{attacker's desired address}} \\
\keywordfont{22:} & \pumod{y}{24} \\
\keywordfont{23:} & \pifjump{0=0}{10} \\
\keywordfont{24:} & \textit{copy memory contents from}~z~ \\
& \vdots \\
\end{array}
\right.
\end{array}
\]
Here, our original program (not the untrusted $\asprg$) dereferences memory and makes use of indirect control flow transfer.
The attacker identifies a sequence of commands in the trusted portion of the program, and sets things up in a way so that unauthorized memory is copied into a variable that the attacker can later access once control is returned to the untrusted code. This should remind you of a \emph{return-oriented programming} (ROP) attacks~\cite{Hovav07} that you learned about in 15-213. If we assume that the attacker knows the text of our program, then it is possible for them to identify ``gadgets'' in \emph{code that we wrote} to do their bidding. But this crucially relies on the ability to change control flow using indirection so that commands are executed in the order needed by the attacker to carry out their goals.

We discussed addressing this with a very similar approach to what we did for SFI. We built a ``code sandbox'' between commands at $\pc_l$ and $\pc_h$. Then we can rewrite indirect jump commands to ensure that their target always lies within these bounds.
\begin{equation}
\textit{Rewrite all}~~\pifjump{\ivr}{\astrm}~~\textit{commands as}~~\pifjump{\ivr}{(\astrm \bitand \pc_h) \bitor \pc_l}
\end{equation}
This is a form of \emph{control flow integrity} (CFI)~\cite{Abadi2009}, a technique for enforcing a broad class of safety properties that place limits on the allowed control flow paths in a program. But this sort of policy doesn't give us much flexibility in terms of what kind of control flow we might want to restrict the untrusted code to, because any sequence within the code sandbox is considered valid.

Today we'll start by looking at a more granular type of control flow policy that takes into account a control flow graph, and can be used to ensure that the target program executes only sequences of instructions corresponding to paths in the graph.

\section{Finer-grained control-flow safety}
But what if this isn't the case? Suppose that we want to enforce other invariants on untrusted code, such as that they do not modify a protected variable under certain conditions. So for example if $x$ is negative, then we want to jump over any assignment to $x$. We make the following replacements, among others:
\begin{center}
\begin{tabular}{lcl}
$
\begin{array}{ll}
\keywordfont{i:} & x := \astrm
\end{array}
$
&
\ \ \ \ becomes\ \ \ \ 
&
$
\begin{array}{rl}
\keywordfont{i:} & \pifjump{x < 0}{i+2} \\
\keywordfont{i+1:} & x := \astrm
\end{array}
$
\end{tabular}
\end{center}
Now if we use the coarse-grained CFI policy from before, can we actually enforce the policy using this approach? It would seem not, at least as long as the attacker knows that this is how we will attempt to do so. The problem arises because of the fact that according to the coarse-grained CFI policy, any address in the untrusted code is an allowed target of a jump. So it is perfectly acceptable (according to the coarse-grained policy) for the attacker to jump directly past the inlined check.

To see this more concretely, consider the following proof-of-concept attack code. Suppose that the attacker wants to set $x$ to 0 regardless of what its value is before the untrusted code executes. This obviously violates the policy, and to accomplish it the attacker will provide a program that takes the inline enforcement code into account. So after providing the code on the left,
\begin{center}
\begin{tabular}{lcl}
$
\begin{array}{rl}
\keywordfont{i-1:} & \pifjump{0=0}{i+1} \\
\keywordfont{i:} & x := 0
\end{array}
$
&
\ \ \ \ becomes\ \ \ \ 
&
$
\begin{array}{rl}
\keywordfont{i-1:} & \pifjump{0=0}{(i+1 \bitand s_h) \bitor s_l} \\
\keywordfont{i:} & \pifjump{x < 0}{i+2} \\
\keywordfont{i+1:} & x := 0
\end{array}
$
\end{tabular}
\end{center}
In short, the attacker sets up the program to jump directly over the enforcement code.

\paragraph{Enforcing the control flow graph.} To address this, we can rely on the control flow graph (CFG) of the untrusted code. Recall from lecture 5 that a program's CFG is a graph that encodes all of the possible valid transitions between commands in the program. In this case, we will obtain a control flow graph for the original untrusted program\footnote{Obtaining the CFG for an arbitrary program with indirect jumps is a difficult problem indeed. It may not always be possible to do so, and we will come back to this in later lectures. For now, we will just assume that we have obtained the correct CFG for $\asprg$ by some unknown means.}, and ensure that the program instrumented with inline safety checks follows the same CFG modulo any checks.

So in this case, the original control flow graph is given on the left of the diagram below. Obviously it is not the case that $0 \ne 0$, so the edge from \keywordfont{i-1} to \keywordfont{i} is never taken. After the invariant instrumentation is inserted, the correct translation of the control flow, preserving the relative edges from the original CFG, is shown on the right. The instrumentation replaced the instruction originally at \keywordfont{i} with an inline check, and shifted all of the subsequent instructions up by one address. So this moves \keywordfont{i} to \keywordfont{i+1}, and \keywordfont{i+1} to \keywordfont{i+2}. To make this clear in the diagram, the nodes and edges corresponding to instrumentation are marked in blue.
\begin{center}
\begin{tikzpicture}
[
  highlight/.style={draw=blue, text=blue},
  shorten >=1pt,
  node distance=2cm,
  on grid,
  auto,
  /tikz/initial text={},
  font=\footnotesize,
  thick
]
 \node[state,initial above,inner sep=1pt,minimum size=0.75cm] (l0)  {\keywordfont{i-1}};
 \node[state,inner sep=1pt,minimum size=0.75cm,yshift=1em] (l1) [below=of l0] {\keywordfont{i}};
 \node[state,inner sep=1pt,minimum size=0.75cm,yshift=1em] (l2) [below=of l1] {\keywordfont{i+1}};
 \node[state,draw=none,inner sep=1pt,minimum size=0.75cm,yshift=3.15em] (l3) [below=of l2] {};

  \path[->] 
    (l0) edge node [right] {$0 \ne 0$} (l1)
    (l1) edge node [right] {} (l2)
    (l0) edge [bend right] node [left] {$0 = 0$} (l2);
\end{tikzpicture}
\hspace{3em}
\begin{tikzpicture}
[
  highlight/.style={draw=blue, text=blue},
  shorten >=1pt,
  node distance=2cm,
  on grid,
  auto,
  /tikz/initial text={},
  font=\footnotesize,
  thick
]
 \node[state,initial above,inner sep=1pt,minimum size=0.75cm] (l0)  {\keywordfont{i-1}};
 \node[state,fill=blue!30,inner sep=1pt,minimum size=0.75cm,yshift=1em] (l1) [below=of l0] {\keywordfont{i}};
 \node[state,inner sep=1pt,minimum size=0.75cm,yshift=1em] (l2) [below=of l1] {\keywordfont{i+1}};
 \node[state,inner sep=1pt,minimum size=0.75cm,yshift=1em] (l3) [below=of l2] {\keywordfont{i+2}};

  \path[->] 
    (l0) edge node [right] {$0 \ne 0$} (l1)
    (l1) edge [blue] node [right] {$x \ge 0$} (l2)
    (l0) edge [bend right] node [left] {$0 = 0$} (l3)
    (l1) edge [blue,bend left=70] node [right] {$x < 0$} (l3)
    (l2) edge node [right] {} (l3);
\end{tikzpicture}
\end{center}
Now to correctly enforce the safety policy that the CFG on the right is respected by the code, the original jumps are rewritten accordingly.
\[
\begin{array}{rl}
\keywordfont{i-1:} & \pifjump{0=0}{i+2} \\
\keywordfont{i:} & \pifjump{x < 0}{i+2} \\
\keywordfont{i+1:} & x := 0
\end{array}
\]
By enforcing the original control flow of the program, after taking any added instrumentation into account with dealing with instruction addresses, we prevented the attacker from bypassing our inlined safety policy enforcement. But notice that it didn't really matter what control flow graph we started out with. It could have been arbitrary, perhaps completely different from the actual CFG of the original program, and we could still enforce it by inserting and replacing conditional jumps.

\section{Security automata}

The point mentioned at the end of the previous section raises some interesting possibilities. What if we want to enforce a more general safety property that takes aspects of control flow and program state into account? For example, suppose that our language from the previous two sections has the ability to make three types of system calls, \psend and \precv from the network and \pread from a local file. Then we quite naturally might want to enforce a safety policy on untrusted code which says that \psend cannot be called after \pread.

We can encode such a policy using a \emph{security automaton}~\cite{Schneider2000}. Depicted below is a security automaton for the safety policy ``no send after read''. The states are abstract in the sense that they do not reflect anything about the state of the program or what it is currently doing. Rather, they represent the state at which the policy is currently in. The transitions reflect facts about program state that must be true in order for the automaton to transition. In this case, $\pc$ denotes the current program counter, and $\pcderef$ its contents. So for example $\pcderef \ne \pread$ corresponds to states in which the current instruction pointed to by the program counter is not \pread.
\begin{center}
\scalebox{1.2}{%
\begin{tikzpicture}
[
  highlight/.style={draw=blue, text=blue},
  shorten >=1pt,
  node distance=2cm,
  on grid,
  auto,
  /tikz/initial text={},
  font=\footnotesize,
  thick
]
 \node[state,thick,initial left,inner sep=1pt,minimum size=0.75cm] (l0) {};
 \node[state,thick,inner sep=1pt,minimum size=0.75cm] (l1) [right=3cm of l0] {};

  \path[->] 
    (l0) edge [thick,loop above] node [above] {$\pcderef \ne \pread$} (l0)
    (l0) edge [thick] node [above] {$\pcderef = \pread$} (l1)
    (l1) edge [thick,loop above] node [above] {$\pcderef \ne \psend$} (l1);
\end{tikzpicture}
}
\end{center}
Notice that the only arrow going out of the rightmost state is a self-loop labeled $\pcderef \ne \psend$. There are no accepting states in a security automaton, and the way to interpret them is that as long as the automaton can transition from some arrow in its current state, then the policy has not been violated. So in this case, if the current policy state were the rightmost one, and the program entered into a state where $\pcderef = \psend$, then there would be no arrow to transition from and the policy would become violated.

Another way to think about it is that there is a ``hidden'' error state which corresponds to the policy being violated. Every node has a transition to the error state on the condition that is the negation of all other outgoing transitions from that state, as shown in the diagram below.
\begin{center}
\scalebox{1}{%
\begin{tikzpicture}
[
  highlight/.style={draw=blue, text=blue},
  shorten >=1pt,
  node distance=2cm,
  on grid,
  auto,
  /tikz/initial text={},
  font=\footnotesize,
  thick
]
 \node[state,thick,inner sep=1pt,minimum size=0.75cm] (l0) {};
 \node[state,thick,inner sep=1pt,minimum size=0.75cm,yshift=2cm] (l1) [right=3.5cm of l0] {};
 \node[state,thick,inner sep=1pt,minimum size=0.75cm] (l2) [below=1cm of l1] {};
 \node[state,thick,inner sep=1pt,minimum size=0.75cm,draw=none] (l3) [right=3.5cm of l0] {\vdots};
 \node[state,thick,inner sep=1pt,minimum size=0.75cm,yshift=-2cm] (l5) [right=3.5cm of l0] {};
 \node[state,thick,inner sep=1pt,minimum size=0.75cm] (l4) [above=1cm of l5] {};
 \node[state,thick,inner sep=1pt,minimum size=0.75cm,fill=red] (l6) [left=3.5cm of l0] {$\errstate$};

  \path[->] 
    (l0) edge [thick] node [below] {$P_1$} (l1)
    (l0) edge [thick] node [below] {$P_2$} (l2)
    (l0) edge [thick] node [below,xshift=1ex] {$P_{n-1}$} (l4)
    (l0) edge [thick] node [below] {$P_{n}$} (l5)
    (l0) edge [thick] node [above] {$\lnot(P_1 \lor \cdots \lor P_n)$} (l6);
\end{tikzpicture}
}
\end{center}
These definitions are equivalent, and we will continue using the convention that does not explicitly list the error state as this will reduce clutter in our diagrams.

\begin{definition}[Security automaton\cite{Schneider2000}]
\label{def:sa}
A security automaton is a nondeterministic state machine that consists of the following components:
\begin{itemize}
\item a countable set $O$ of automaton states,
\item a countable set $O_0 \subseteq O$ of initial states,
\item a countable set $\Sigma$ of transition symbols,
\item a transition relation $\delta \subseteq O \times \powerset{\Sigma} \times O$ between automaton states and sets of transition symbols.
\end{itemize}
We will assume that sets of automaton states are represented by formulas $P,Q,\ldots$ that can be evaluated on transition symbols to determine whether the symbol is in the set.
Given a sequence of transition symbols $\sigma = \sigma_0,\sigma_2,\ldots$, we say that the autoamton accepts $\sigma$ if and only if there is a corresponding sequence of states $o = o_0,o_1,\ldots$ such that for each pair $\sigma_{i},\sigma_{i+1}$ in $\sigma$,
\begin{itemize}
\item there is a corresponding pair $o_{i},o_{i+1}$ of states in $o$,
\item and there exists $(o_{i}, P, o_{i+1}) \in \delta$ where $P(\sigma_i)$ is true.
\end{itemize}
In other words, a trace is only accepted if there is a corresponding run of the automaton that always follows the transition function.
\end{definition}

Definition~\ref{def:sa} formally defines security automata in terms of a set of states $O$ and transition symbols $\mathcal{S}$. We will generally assume that $\mathcal{S}$ is the set of all program states, so that we can describe program traces as being accepted or not by a security automaton. This also implies that sets of states in the transition relation are defined in terms of formulas on program states, which we have already studied extensively.

\subsection{Enforcing security automata policies}

The primary means of enforcing policies defined using security automata is with a \emph{reference monitor} (RM). The RM is a mechanism that examines the program as it executes, using information about the current and past states to decide whether the policy has been violated. This is done according to Definition~\ref{def:sae}, and was sketched out at the beginning of this section.

\begin{definition}[Security automaton enforcement]
\label{def:sae}
Let $O_c$ be the current set of states that the security automaton is in. Then for each step that the program is about to take resulting in new program state $\omega$, the reference monitor does one of two things.
\begin{enumerate}
\item For each state $o \in O_c$ that the automaton can transition from, the states $\delta(o,P,o')$ for all transition edges where $P(\omega)$ is true are added to the new automaton states.
\item If the automaton cannot transition from any of its current states, then the program is not allowed to enter state $\omega$ and the reference monitor takes remedial action.
\end{enumerate}
\end{definition}

As long as the policy is not violated, then the RM allows the program to continue executing as it otherwise would. If the policy is violated, then the RM intervenes on the program execution to take some remedial action. This could mean simply aborting the execution, or something less drastic that prevents harm in other ways.

\paragraph{Necessary assumptions.} As pointed out by Schneider in his seminal work on security automata~\cite{Schneider2000}, there are several assumptions that one must make in order to enforce these policies effectively with a reference monitor. First, the reference monitor needs to simulate the execution of the automaton as the program runs, so it must keep track of which state the policy is in on the actual hardware running the program. This means that the automaton cannot require an unbounded amount of memory, so automata that have an infinite number of states are not in general enforceable.

Second, the RM must be able to prevent the program from entering a state that would result in a policy violation. This is called \emph{target control}, and is a more subtle issue that it may at first seem. Take for example the policy of ``real-time'' availability, which states that a principal should not be denied a resource for more than $n$ real-time seconds. How could a reference monitor enforce this policy? It might try to predict the amount of time that it takes to remediate a trace that is about to violate the policy, and take action earlier than necessary to prevent the violation. But how does it know that the policy would have actually been violated in this case? Unless the reference monitor can literally stop time, this is not an enforceable policy.

Third, the program under enforcement must not be able to intervene directly on the state of the reference monitor. This is called \emph{enforcement mechanism integrity}, and is crucial for ensuring that the policy defined by the automaton is the one that is actually enforced on the target program. We dealt with an instance of this issue earlier in the lecture, when we used control flow integrity to make sure that inlined safety checks weren't bypassed by indirect jumps. But now that the policy itself has state, the enforcement mechansim must also guarantee that the target program does not make changes to that state or influence it in any way that doesn't follow the automaton transitions.

\paragraph{Inline SA enforcement.} One approach to implementing security automata enforcement uses inlined checks to update and maintain state set aside to simulate the automaton. If we assume that formulas on SA transitions are formulas over program states, and there are $N$ security automata states, then we can set aside a region of $N$ memory cells at addresses $a_{\textit{sa}}$ through $a_{\textit{sa}}+N$ to hold the current state of the automaton. If $\pderef{a_{\textit{sa}}+i}$ is non-zero, then we assume that the automaton has entered into state $i$, and otherwise not.

Next we need to implement the transition function, updating the contents of $\pderef{a_{\textit{sa}}} - \pderef{a_{\textit{sa}}+N}$ to simulate the automaton. Suppose that the automaton has an edge from states $i$ to $j$ labeled with formula $\ausfml$. Then for each instruction in the program we compute the verification condition of (\ref{eq:savc}).
\begin{equation}
\label{eq:savc}
\dbox{\asprg}{\lnot\ausfml}
\end{equation}
If (\ref{eq:savc}) is valid before executing $\asprg$, then it means that all traces after executing $\asprg$ will satisfy $\lnot\ausfml$. On the other hand, if it is not valid, then at least one trace of $\asprg$ may satisfy $\ausfml$. This means that we need to insert a check whenever Eq~\ref{eq:savc} is not valid.

What check do we insert? At runtime, we will be in a particular state $\omega$. We want to know if after executing $\asprg$, $\ausfml$ will be true, and if it is, then update the state of the automaton. We can accomplish this by simply checking that $\omega \models \dbox{\asprg}{\ausfml}$. Of course, we will want to use axioms to remove the box modality so that the check is actually in terms of arithmetic, and can be easily evaluated.

So we insert instrumentation immediately before $\asprg$ that checks $\pderef{a_{\textit{sa}}+i} \ne 0 \land \dbox{\asprg}{\ausfml}$, and if it is true then sets $\pderef{a_{\textit{sa}+j}}$ to a non-zero value. Then for each state $i$ in the SA, we compute similar checks for transition to the ``error state''. If $i$ has outgoing edges labeled $P_1, \ldots, P_n$, we insert a check for:
\begin{equation}
\pderef{a_{\textit{sa}}+i} \ne 0 \land \dbox{\asprg}{\lnot(P_1 \lor \cdots \lor P_n)}
\end{equation}
If this check passes, it means that the automaton cannot transition from state $i$. If this holds for every state in the automaton, then the instrumentation aborts execution.

The instrumentation described so far only addresses updates to the SA state. We must also take steps to ensure the integrity of the inlined mechanism, and there are two sources of vulnerability.
\begin{itemize}
\item The contents of $\pderef{a_{\textit{sa}}} - \pderef{a_{\textit{sa}}+N}$ must not be modified by any part of the program except the inserted instrumentation. Applying software fault isolation to the untrusted instructions can ensure that this aspect of integrity holds.
\item The inserted instrumentation could be subverted by indirect control flow. Enforcing CFI on the untrusted code using the original control flow graph ensures that this will not happen.
\end{itemize}
This is sufficient to implement a basic inlined security automaton enforcement mechanism. However, it may impose a severe performance overhead due to all the safety checks.

\section{Dynamic instrumentation}
We have been discussing policy enforcement in a somewhat idealized model, where we assume that programs are given to us as source code in a simple language with few instructions. In the ``real world'' this is not usually the case, and we may be forced to deal with large untrusted programs given to us to execute at runtime, and possibly without source code. So we must find a way to enforce policies on bytecode, and presumably fast lest we introduce unacceptable latency into the system.

Suppose that we wish to implement the inline security automata enforcement scheme from the previous section by changing the instructions throughout the program prior to running it. This seems like a reasonable approach, because the scheme just requires that we check verification conditions on each instruction and replace them when necessary. All that we need to assume is the ability to identify instructions, and compute verification conditions.

\subsection{Challenges for static instrumentation}
But bytecode programs on modern architectures like x86 and AMD64/Intel 64 are extremely difficult to reason about statically, and it may not even be possible to identify which instructions the program will end up executing. One practical issue is the fact that programs can generate new instructions by writing to memory, and then use an indirect jump to begin executing the newly-written code. This can be mitigated by the operating system with a \emph{Write XOR Execute} policy, which ensures that any page of memory may be either writeable or executable, but not both. This is effective, but makes some functionality extremely difficult to implement such as language interpreters that do on-the-fly compilation and optimization.

Even with Write XOR Execute, the presence of indirect control flow and variable-length instruction encoding makes it impossible to tell which instructions will actually be executed. The program can do an arbitrarily complicated computation to derive a target address in existing code, so that the static analysis is unable to determine where execution will resume after a jump. If the target address is in the middle of an existing instruction, it may result in a completely different program being executed. Consider the following example, taken from \cite{Hovav07}.
\begin{equation}
\label{eq:bytecode1}
\begin{array}{lcl}
\mathit{Bytecode} & & \mathit{Instruction} \\
\hline
\verb'f7 c7 07 00 00 00' & & \verb'test $0x00000007, %edi' \\
\verb'0f 95 45 c3' & & \verb'setnzb -61(%ebp)'
\end{array}
\end{equation}
This code is taken from the entry point of an encryption routine in the GNU C library, often referred to as simply libc. If execution begins one byte after the entry point of (\ref{eq:bytecode1}), a completely different program is executed.
\begin{equation}
\label{eq:bytecode2}
\begin{array}{lcl}
\mathit{Bytecode} & & \mathit{Instruction} \\
\hline
\verb'c7 07 00 00 00 0f' & & \verb'movl $0x0f000000, (%edi)' \\
\verb'95' & & \verb'xchg %ebp, %eax' \\
\verb'45' & & \verb'inc %ebp' \\
\verb'c3' & & \verb'ret'
\end{array}
\end{equation}
Importantly this implies that given a sequence of bytecodes, there are numerous possible programs that could end up being executed depending on which addresses are targeted by indirect jumps. In order to instrument the right one, a static analysis needs to determine what these addresses will be, and this is an undecidable problem in general. Moreover, it could be that information not available statically, such as network packets, are used in part to compute target addresses, adding yet another very plausible complication for static instrumentation in this setting.

\subsection{Instrumenting with just-in-time compilation}
Perhaps a better approach given these challenges is to delay ``code discovery'' until the program is actually running. This is helpful for many reasons.
\begin{itemize}
\item If the program generated instructions in memory and transferred control to them, we no longer need to infer what those instructions will be. We can simply wait until the program has already written them, and instrument them immediately before the control transfer.
\item If a program executes an indirect jump, we do not need to predict what the target address will be. We simply wait until immediately before the jump is executed, at which point the target address will be stored in memory or a register, and begin instrumenting the target of the jump.
\item Some other cases that we have not discussed are handled similarly, such as libraries that are loaded after the program begins executing. In each such case, the instrumentation is delayed until immediately before the instructions in question begin executing, at which point all of the necessary information is available.
\end{itemize}
The obvious drawback to this approach is the fact that we need to examine the execution as it unfolds, rewriting instructions whenever necessary as dictated by the policy.

\paragraph{Just-in-time compilation.} A successful and widely-deployed approach to mitigate the performance penalty imposed by such a scheme is called \emph{just-in-time (JIT) compilation}~\cite{Aycock2003}. The key insight behind JIT compilation is to increase the granularity at which the instrumenter examines code at runtime, looking at ``chunks'' of instructions rather than individual ones. 

Increasing the granularity in this way allows the instrumenter to compile instruction chunks, with their instrumentation included, on the fly into optimized code that is then executed directly. Further performance enhancements can then be layered on top of this basic approach, such caching previously-compiled chunks to save redundant work, as well as more aggressive optimizations to sequences of chunks that end up being executed more often.

The question then becomes what constitutes a chunk. Larger chunks will generally create more opportunities for optimization, and because more of the instructions are dealt with each time, require fewer (expensive) calls to the compiler. However, this tendency is limited by the fact that if a chunk crosses an indirect control flow instruction, then we run into exactly the same problems we are trying to avoid with dynamic instrumentation in the first place. Even if our chunks cross direct, predictable control flow branches, then we run the risk of doing unnecessary compilation and instrumentation by processing multiple branches when the execution will only end up following one of them.

The typical approach is to use \emph{basic blocks} as chunks. A basic block is a contiguous sequence of instructions that ends in a control flow transfer instruction (e.g., \verb'jmp', \verb'ret', \verb'call', \ldots). For example, the sequence of instructions in (\ref{eq:bytecode2}) is a basic block because it ends with a \verb'ret' instruction, which transfers control to the instruction pointed to by the return address on the stack. On the other hand, (\ref{eq:bytecode1}) is not a basic block because it does not end in such an instruction.

Using basic blocks as chunks, the instrumenter will begin scanning a sequence of bytecodes until it reaches a control transfer instruction. It will then instrument each of the instructions in the basic block as prescribed by the policy, compile the resulting instructions, and execute them. However, it must ensure that it regains control when the basic block is finished executing. It then begins scanning instructions again at the bytecodes pointed to by the instruction pointer, repeating the process all over again. In this way we can be sure that exactly the code that is executed is instrumented according to the policy.

\paragraph{A look ahead: Pin.} In your next lab, you will make use of an instrumentation tool called Pin~\cite{Luk2005} that is based on JIT compilation. Pin is under ongoing development by Intel, and is widely used in industry as well as in academic research. It simplifies the task of instrumenting binaries at runtime by providing a high-level API for both inspecting and instrumenting sequences of instructions at runtime. 

\begin{figure}[t]
\centering
\includegraphics[width=0.5\textwidth]{pin.pdf}
\caption{Pin software architecture (from~\cite{Luk2005}).}
\label{fig:pin}
\end{figure}

To see why this is helpful, consider the task of instrumenting an x86 binary to prevent writes to certain portions of memory. To do so, we must rewrite all instructions that can change memory with instrumentation to stop the unwanted writes. Which instructions can change memory? The obvious ones are \verb'mov', \verb'push', \verb'pop', \verb'lea', \verb'xchg', and perhaps a few others. But what about the many variants of \verb'mov', such as \verb'movsb', \verb'movsw', \verb'movz', \verb'movzx'? Do the other instructions have variants as well, and how can we be sure that we've covered each one? Pin simplifies things for us by providing \verb'INS_IsMemoryWrite(ins)', which returns true if \verb'ins' can update memory.

Figure~\ref{fig:pin} shows the architecture of Pin. Users interact with it by writing a ``Pintool'', which is a conventional C or C++ program that makes use of the Pin inspection and instrumentation API. To run a compiled program under the pintool's instrumentation, the program's binary is passed to Pin along with the compiled pintool. Pin then takes care of just-in-time compiling the target program, and can invoke callbacks to the pintool as requested for inspection, or rewrite instructions as requested for instrumentation. As execution proceeds, Pin's optimization routines run in tandem to progressively optimize the compiled code.

You will learn more about the specifics of the Pin API in the handout for the next lab, and get hands-on experience using it to implement SFI as well as a security automaton policy. For more detailed information on how Pin works, consult the original paper~\cite{Luk2005}.

\bibliographystyle{abbrv}
\bibliography{bibliography}
\end{document}